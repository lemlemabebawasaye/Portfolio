<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Lemlem Abebaw Asaye</title>
  <meta name="author" content="Lemlem Abebaw Asaye">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'%3E%3Ctext y='.9em' font-size='90'%3EðŸŒ%3C/text%3E%3C/svg%3E">
  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&family=Playfair+Display:wght@400;700&display=swap" rel="stylesheet">
</head>
<body>
  <!-- Navigation Menu -->
  <nav class="topnav">
    <a href="#home">Home</a>
    <a href="#Award">Award</a>
    <a href="#blog">Blog</a>
    <a href="#projects">Projects</a>
    <a href="#publications">Publications</a>
  </nav>

  <!-- Home Section -->
  <section id="home" class="section">
    <div class="container">
      <div class="intro">
        <div class="intro-text">
          <h1 class="name">Lemlem Abebaw Asaye</h1>
          <p class="bio">
            Lemlem Abebaw Asaye completed her Bachelors degree in Civil Engineering.
          </p>
          <div class="social-links">
            <a href="mailto:lemlemabebawasayegetahun@gmail.com" class="social-link">Email</a>
            <a href="https://scholar.google.com/citations?user=89G8cNwAAAAJ&hl=en&&authuser=1" class="social-link">Google Scholar</a>
            <a href="https://github.com/lemlemabebawasaye/" class="social-link">Github</a>
            <a href="https://www.linkedin.com/in/lemlem-abebaw-asaye" class="social-link">LinkedIn</a>
          </div>
        </div>
        <div class="profile-image">
          <img src="lemlem.jpeg" alt="Profile Photo" class="hoverZoomLink">
        </div>
      </div>
    </div>
  </section>

  <!-- Publications Section -->
  <section id="publications" class="section">
    <div class="container">
      <h2>Research</h2>
      <p class="subtitle">Representative papers are <span class="highlight">highlighted</span>.</p>
      <div class="publication-list">
        <!-- Publication Entries -->
        <div class="publication-entry">
          <img src="images/gmm.png" alt="GMM">
          <div class="publication-info">
            <a href="https://www.mdpi.com/2227-9717/13/2/445">
              <h3>Predicting and Understanding Emergency Shutdown Durations Level of Pipeline Incidents Using Machine Learning Models and Explainable AI</h3>
            </a>
            <p><strong>Lemlem Abebaw</strong>,  Chau Le,  Ying Huang, Trung Q. Le, Om Prakash Yadav, and Tuyen Le </p>
            <p><em>MDPI Processes, 2025</em></p>
            <p>Pipeline incidents pose significant concerns due to their potential environmental, economic, and safety risks, emphasizing the critical 
		need to understand and manage this vital infrastructure. While existing studies predominantly focus on the causes of pipeline incidents
		and failures, few have investigated the consequences, such as shutdown duration, and most lack comprehensive models capable of accurately
		predicting and providing actionable insights into the risk factors. This study bridges this gap by employing machine learning (ML)
		techniques, including Random Forest and Light Gradient Boosting Machine (LightGBM), for classifying pipeline incidentsâ€™ emergency shutdown 
		duration levels. These techniques are specifically designed to capture complex, nonlinear patterns and interdependencies within the data, 
		addressing the limitations of traditional linear approaches. The proposed model has further enhanced with Explainable AI (XAI) techniques, 
		such as Shapley Additive exPlanations (SHAP) values, to improve interpretability and provide insights into the factors influencing shutdown durations.
		Historical incident data, collected from the Pipeline and Hazardous Materials Safety Administration (PHMSA) from 2010 to 2022, were utilized to examine 
		the risk factors. K-Fold Cross-Validation with 5 folds was employed to ensure the modelâ€™s robustness. The results demonstrate that the LightGBM model achieved 
		the highest accuracy of 75.0%, closely followed by Random Forest at 74.8%. The integration of XAI techniques provides actionable insights into key factors
		such as pipeline material, age, installation layout, and commodity type, which significantly influence shutdown durations. These findings underscore the practical
		implications of the proposed approach, enabling pipeline operators, emergency responders, and regulatory authorities to make informed decisions that optimize 
		resource allocation and mitigate risks effectively.
</p>
    </div>
  </div>  
	
  <!-- WCAM: Wavelet Convolutional Attention Module  -->
  <div class="publication-entry">
    <img src="images/wcam.png"  alt="safs_small" width="220" height="140" style="border-style: none">
    <div class="publication-info">
      <a href="https://ieeexplore.ieee.org/document/10500182">
        <h3>WCAM: Wavelet Convolutional Attention Module </h3>
      </a>
      <p><strong>Simegnew Alaba </strong> and John E. Ball</p>
      <p><em>IEEE SoutheastCon,  April 24,</em>, 2024</p>
      <p>Convolutional neural networks (CNNs) excel at extracting complex features from images,
	but conventional CNNs process entire images uniformly, which can be inefficient since not
	all regions are equally important. To address this, the Wavelet Convolutional Attention Module (WCAM) 
	focuses CNNs on crucial features, using wavelets' multiresolution and sparsity to guide efficient
	      downsampling without losing details.</p>
    </div>
  </div>  
	
	
<!-- Emerging Trends in Autonomous Vehicle Perception: Multimodal Fusion for 3D Object Detection  -->
  <div class="publication-entry">
    <img src="images/multihead.png"  alt="safs_small" width="220" height="140" style="border-style: none">
    <div class="publication-info">
      <a href="https://www.mdpi.com/2032-6653/15/1/20">
        <h3>Emerging Trends in Autonomous Vehicle Perception: Multimodal Fusion for 3D Object Detection </h3>
      </a>
      <p><strong>Simegnew Alaba </strong>, ALi C Gurbuz, and John E. Ball</p>
      <p><em>World Electric Vehicle Journal, January 7,</em>, 2024</p>
      <p>This work provides an exhaustive review of multimodal fusion-based 3D object detection methods,
	      focusing on CNN and Transformer-based models. It underscores the necessity of equipping fully 
	      autonomous vehicles with diverse sensors to ensure robust and reliable operation. 
	      The survey explores the advantages and drawbacks of cameras, LiDAR, and radar sensors.
	      Additionally, it summarizes autonomy datasets and examines the latest advancements in multimodal fusion-based methods.
              The work concludes by  highlighting the ongoing challenges, open issues, and potential directions for future research.</p>
    </div>
  </div>

  <!-- SmRNet: Scalable Multiresolution Feature Extraction Network -->
  <div class="publication-entry">
    <img src="images/SmrNet.png" alt="SmRNet Paper Thumbnail" style="width:220px; height:140px;">
    <div class="publication-info">
      <a href="https://ieeexplore.ieee.org/document/10389571">
        <h3>SmRNet: Scalable Multiresolution Feature Extraction Network</h3>
      </a>
      <p><strong>Simegnew Alaba </strong>, John E. Ball</p>
      <p><em>IEEE ICECET, Cape Town-South Africa, November 16-17</em>, 2023</p>
      <p>This work addresses the challenge of creating a lightweight CNN with high accuracy. It replaces pooling and 
      stride operations with discrete wavelet transform (DWT) and inverse wavelet transform (IWT) as downsampling 
      and upsampling operators. This preserves lost details during downsampling, leveraging wavelets' lossless property.</p>
    </div>
  </div>
  <div class="publication-entry">
      <img src="images/zsd_diagram.png" alt="safs_small" width="220" height="140" style="border-style: none">
       <div class="publication-info">
          <a href="https://ieeexplore.ieee.org/document/10336951">
                <h3>A Zero Shot Detection Based Approach for Fish Species Recognition in Underwater Environments</h3>
             </a>
           <p>Chiranjibi Shah, M. M. Nabi, <strong>Simegnew Yihunie Alaba</strong>, Jack Prior, Ryan Caillouet, Matthew D. Campbell, Farron Wallace, John E. Ball, Robert Moorhead</p>
           <p><em>IEEE Ocean Conference & Exposition, Biloxi, MS, USA, September 25--28</em> , 2023</p>
            <p>Utilizing Zero-Shot Detection for Fish Species Recognition (ZSD-FR) in underwater environments, this approach can  identify and locate objects, even for classes not seen during training.
		      Generative models, like GANs, generate samples for unseen classes by leveraging semantics learned from seen classes. </p>
        </div>
     </div>

     <div class="publication-entry">
        <img src="images/AL_net.png" alt="safs_small" width="220" height="140" style="border-style: none">
        <div class="publication-info">
           <a href="https://ieeexplore.ieee.org/document/10337403">
              <h3>Probabilistic Model-based Active Learning with Attention Mechanism for Fish Species Recognition</h3>
           </a>
          <p> M. M. Nabi, Chiranjibi Shah,  <strong>Simegnew Yihunie Alaba</strong>, Jack Prior, Matthew D. Campbell, Farron Wallace,  Robert Moorhead, John E. Ball</p>
          </p> <em>IEEE Ocean Conference & Exposition Biloxi, MS, USA, September 25--28</em>, 2023</p>
          <p> This work introduces a deep-learning fish detection and classification model with the Convolutional 
		      Block Attention Module (CBAM) for enhanced detection. It employs a cost-efficient Deep Active Learning 
		      approach to select informative samples from unlabeled data. Probabilistic modeling, using mixture density networks,
		      estimates probability distributions for localization and classification. </p>
        </div>
     </div>

    <div class="publication-entry">
        <img src="images/fish_survey_conference.jpg" alt="safs_small" width="220" height="140" style="border-style: none">
        <div class="publication-info">
           <a href="https://ieeexplore.ieee.org/document/10337410">
           <h3>Optimizing and Gauging Model Performance with Metrics to Integrate with Existing Video Surveys</h3>
          </a>
          <p>Jack Prior, <strong>Simegnew Yihunie Alaba</strong>,  Farron Wallace, Matthew D. Campbell,  Chiranjibi Shah,  M. M. Nabi,  Paul F. Mickle,
		    Robert Moorhead, John E. Ball</p>
          <p> <em>IEEE Ocean Conference & Exposition Biloxi, MS, USA, September 25--28</em>, 2023 </p>
              
          <p>Automated deep-learning models offer efficient fish population monitoring in baited underwater video sampling.
		      Precise results are essential, and otolith age-reader comparison helps evaluate model performance. 
		      Increasing annotations improve most species' performance, but challenges remain with occlusion, turbidity, 
		      schooling species, and cryptic appearances. Red Snapper's multiple-year evaluation highlights location, time, 
		      and environmental considerations. Adapting active learning algorithms can enhance model efficiency. 
		      These quality assessment methods aid in integrating automated methods with manual video counts. </p>
        </div>
    </div>

    <div class="publication-entry">
        <img src="images/multisensor-fusion.png" alt="fast-texture" width="220" height="140">
        <div class="publication-info">
           <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12540/1254005/Multi-sensor-fusion-3D-object-detection-for-autonomous-driving/10.1117/12.2663424.short?SSO=1">
              <h3>Multi-sensor fusion 3D object detection for autonomous driving</h3>
            </a>
           <p><strong>Simegnew Yihunie Alaba</strong>, John E. Ball</p>
           <p><em>Autonomous Systems: Sensors, Processing and Security for Ground, Air, Sea, and Space Vehicles and Infrastructure, Orlando, 
		      FL, USA, April 30 â€“ May 4</em>, 2023</p>
           <p>  A multi-modal fusion 3D object detection model is proposed for autonomous driving, aiming to leverage the strengths of
		       LiDAR and camera sensors.</p>
        </div>
      </div>

     <div class="publication-entry">
       <img src="images/semi-supervised.png" alt="fast-texture" width="220" height="140">
       <div class="publication-info">
	   <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12543/125430P/Semi-supervised-learning-for-fish-species-recognition/10.1117/12.2663422.short">
              <h3>Semi-supervised learning for fish species recognition</h3>
           </a>
          <p><strong>Simegnew Yihunie Alaba</strong>, Chiranjibi Shah, M. M. Nabi, John E. Ball, Robert Moorhead, Deok Han, Jack Prior, Matthew D. Campbell, Farron Wallace</p>
          <p><em>Ocean Sensing and Monitoring XV: Machine Learning/Deep Learning 1, Orlando, FL, USA, April 30 â€“ May 4</em>, 2023</p>
          <p> A semi-supervised deep-learning network is developed for fish species recognition, which is crucial for 
		      monitoring fish activities, distribution, and ecosystem management.</p>
        </div>
      </div>

      <div class="publication-entry">
        <img src="images/MI-AFR.png" alt="fast-texture" width="220" height="140">
	<div class="publication-info">
           <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12543/125430N/MI-AFR--multiple-instance-active-learning-based-approach-for/10.1117/12.2663404.short">
              <h3>MI-AFR: multiple instance active learning-based approach for fish species recognition in underwater environments</h3>
            </a>
           <p>Chiranjibi Shah, <strong>Simegnew Yihunie Alaba</strong>, MM Nabi, Ryan Caillouet, Jack Prior, Matthew Campbell, Farron Wallace, John E Ball, Robert Moorhead</p>
           <p><em>Ocean Sensing and Monitoring XV: Machine Learning/Deep Learning 1, Orlando, FL, USA, April 30 â€“ May 4</em>, 2023</p>
              
            <p> This work introduces MI-AFR, an object detection-based approach for fish species recognition, 
		      offering an efficient solution to automate video surveys and reduce processing time and cost.</p>
        </div>
     </div>

     <div class="publication-entry">
       <img src="images/yolov5_enhanced.png" alt="fast-texture" width="220" height="140">
       <div class="publication-info">
           <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12543/125430O/An-enhanced-YOLOv5-model-for-fish-species-recognition-from-underwater/10.1117/12.2663408.short">
              <h3>An enhanced YOLOv5 model for fish species recognition from underwater environments</h3>
            </a>
           <p>Chiranjibi Shah, <strong>Simegnew Yihunie Alaba</strong>, MM Nabi, Jack Prior, Matthew Campbell, Farron Wallace, John E Ball, Robert Moorhead</p>
           <p><em>Ocean Sensing and Monitoring XV: Machine Learning/Deep Learning 1, Orlando, FL, USA, April 30 â€“ May 4</em>, 2023</p>
              
           <p> This work presents YOLOv5, an object detection model tailored for fish species recognition. 
		      It addresses challenges in underwater environments and includes modifications to improve performance,
		      such as depth scaling and a transformer block in the backbone network.</p>
        </div>
     </div>
  
	  
       <div class="publication-entry">
         <img src="images/graphical_abstract.jpg" alt="fast-texture" width="220" height="140">
	 <div class="publication-info">
            <a href="https://ieeexplore.ieee.org/abstract/document/10017184">
              <h3>Deep Learning-Based Image 3-D Object Detection for Autonomous Driving: Review</h3>
              </a>
              <p><strong>Simegnew Yihunie Alaba</strong>, John E. Ball</p>
              <p><em>IEEE Sensors </em>, 2023</p>
              
              <p> This survey focuses on 3-D object detection for autonomous driving using camera sensors.
		      It covers depth estimation techniques, evaluation metrics, and state-of-the-art models.
		      The survey also discusses challenges and future directions in 3-D object detection.</p>
          </div>
        </div>

        <div class="publication-entry">
           <img src="images/sensors.png" alt="fast-texture" width="220" height="140">
	   <div class="publication-info">
              <a href="https://www.mdpi.com/1424-8220/22/24/9577">
                <h3>A Survey on Deep-Learning-Based LiDAR 3D Object Detection for Autonomous Driving </h3>
              </a>
              <p><strong>Simegnew Yihunie Alaba</strong>, John E. Ball</p>
              <p><em>MDPI Sensing and Imaging </em>, 2022
	      </p>
              
              <p>This survey focuses on LiDAR-based 3D object detection for autonomous driving. It covers techniques for handling
		      LiDAR data, 3D coordinate systems, and state-of-the-art methods. Challenges like scale change, sparsity, and 
		      occlusions are also discussed.</p>
            </div>
         </div>

         <div class="publication-entry">
            <img src="images/class-aware.jpg" alt="fast-texture" width="220" height="140">
	    <div class="publication-info">
              <a href="https://www.mdpi.com/1424-8220/22/21/8268">
                <h3>Class-Aware Fish Species Recognition Using Deep Learning for an Imbalanced Dataset</h3>
              </a>
              <p><strong>Simegnew Yihunie Alaba</strong>, John E. Ball</p>
              <p><em> MDPI Sensing and Imaging </em>, 2022
	      </p>
              
              <p>This work tackles fish species recognition using object detection to handle multiple fish in a single image. 
		      The model combines MobileNetv3-large and VGG16 backbones with an SSD detection head and proposes a class-aware 
		      loss function to address the class imbalance. </p>
           </div>
          </div>

          <div class="publication-entry">
             <img src="images/wcnn3d.png" alt="fast-texture" width="220" height="140">
	     <div class="publication-info">
              <a href="https://www.mdpi.com/1424-8220/22/18/7010">
                <h3>WCNN3D: Wavelet Convolutional Neural Network-Based 3D Object Detection for Autonomous Driving</h3>
              </a>
              <p><strong>Simegnew Yihunie Alaba</strong>, John E. Ball</p>
              <p><em>MDPI Sensing and Imaging </em>, 2022</p>
              
              <p>This work introduces a 3D object detection network without pooling operations, using wavelet-multiresolution analysis and
		      frequency filters for enhanced feature extraction. The model incorporates discrete wavelet transforms (DWT) and inverse 
		      wavelet transforms (IWT) with skip connections to enrich feature representation and recover lost details during downsampling.
		      Experimental results demonstrate improved performance compared to PointPillars, with reduced trainable parameters.</p>
             </div>
          </div>
  
          <div class="publication-entry">
             <img src="images/seamapd.png" alt="fast-texture" width="220" height="140">
	     <div class="publication-info">
              <a href="https://drive.google.com/file/d/1Dp9fDjt0KMR_xtRT9pZDEbVuFZpTE_hJ/view">
                <h3>SEAMAPD21: a large-scale reef fish dataset for fine-grained categorization</h3>
              </a>
              <p>OcÃ©ane Boulais, <strong>Simegnew Yihunie Alaba</strong> John E Ball, Matthew Campbell, Ahmed Tashfin Iftekhar, Robert Moorehead, James Primrose, Jack Prior, Farron Wallace, Henry Yu, Aotian Zheng
	      </p>
              <p><em> IEEE CVPR FGVC8: The Eight Workshop on Fine-Grained Visual Categorization</em>, 2021</p>
              
              <p>We introduce SEAMAPD21, a large-scale reef fish dataset from the Gulf of Mexico, vital for fishery monitoring. 
		      This dataset enables efficient automated analysis of fish species, overcoming previous challenges. 
		      SEAMAPD21 includes 90,000 annotations for 130 fish species from 2018 and 2019 surveys.
		      We present baseline experiments and results, available at https://github.com/SEFSC/SEAMAPD21.</p>
          </div>
         </div>

  </div>

  <!-- Blog Section -->
  <section id="blog" class="section">
    <div class="container">
      <h2>Blog</h2>
      <div class="blog-list">
        <div class="blog-entry">
          <a href="https://medium.com/@simonyihunie/a-complete-guide-to-point-cloud-processing-c3fe1c22e28f">A Complete Guide to Point Cloud Processing</a>
        </div>
        <div class="blog-entry">
          <a href="https://medium.com/@simonyihunie/a-comprehensive-guide-to-attention-mechanisms-in-cnns-from-intuition-to-implementation-7a40df01a118">A Comprehensive Guide to Attention Mechanisms in CNNs: From Intuition to Implementation</a>
        </div>
        <div class="blog-entry">
          <a href="https://medium.com/@simonyihunie/installing-intel-realsense-d435i-camera-on-nvidia-jetson-orin-nx-step-by-step-guide-2ef420483064">Installing Intel RealSense D435i Camera on Nvidia Jetson Orin NX: Step-by-Step Guide</a>
        </div>
        <div class="blog-entry">
          <a href="https://medium.com/@simonyihunie/arrays-vs-vectors-in-modern-c-a-detailed-comparison-with-examples-b797fefff7f7">Arrays vs. Vectors in Modern C++: A Detailed Comparison with Examples</a>
        </div>
        <div class="blog-entry">
          <a href="https://medium.com/@simonyihunie/introduction-to-vision-transformers-8adcf4cd9a20">Introduction to Vision Transformers</a>
        </div>
      </div>
    </div>
  </section>

  <!-- Accomplishments Section -->
  <section id="accomplishments" class="section">
    <div class="container">
      <h2>Accomplishments</h2>
      <div class="accomplishment-list">
        <div class="accomplishment-entry">
          <p><strong>VOLUNTEER WORK</strong></p>
          <p>
            <strong>Science Fair Judge for high school students</strong>, Starkville, MS February 22, 2022, and March 7, 2023. <br>
            <strong>Journal Reviewer</strong> January 2021 â€” Present
            Serving as a reviewer for the IEEE Access journal, IEEE Sensors Journal, and IEEE Transactions on Intelligent Transportation Systems journal.
          </p>
        </div>
        <div class="award-entry">
          <p><strong>Award</strong></p>
          <p>
           <a  Best Paper Prsentation Award, 2023. </a><br>
	   <a  Best Paper Prsentation Award, 2024. </a>
          </p>
        </div>
        <div class="award-entry">
          <p><strong>Professional Memberships</strong></p>
          <p>
            <a href="#">American Society of Civil Engineers</a><br>
            <a href="#">Society of Women Engineers</a>
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- Footer -->
  <footer class="footer">
    <div class="container">
      <p>&copy; 2025 Lemlem Abebaw Asaye. All rights reserved.</p>
    </div>
  </footer>
</body>
</html>
